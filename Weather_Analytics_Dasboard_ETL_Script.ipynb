{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0705f6",
   "metadata": {},
   "source": [
    "# 🌦️ Weather Data Analytics Pipeline with AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cc147",
   "metadata": {},
   "source": [
    "## 📌 Step 1: Fetch Raw Weather Data from Open-Meteo API\n",
    "This script:\n",
    "- Installs required packages\n",
    "- Requests hourly and daily data for multiple cities\n",
    "- Processes and merges the data\n",
    "- Formats data (adds `is_day`, separates `date_only`, etc.)\n",
    "- Uploads two CSVs (hourly and daily) to `S3://weather-data-bucket-test1/rawData/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Install required dependencies at runtime\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"boto3\", \"openmeteo_requests\", \"requests_cache\", \"pandas\", \"retry_requests\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26778cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from retry_requests import retry\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "\n",
    "# Setup Open-Meteo API client\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# API URL & Parameters\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "\n",
    "# Add location and coordinates\n",
    "location_map = {\n",
    "    'New Haven': [41.3081, -72.9282],\n",
    "    'New York': [40.7143, -74.006],\n",
    "    'Boston': [42.3584,-71.0598],\n",
    "    'Los Angeles': [34.0522,-118.2437],\n",
    "    'Las Vegas': [36.175,-115.1372],\n",
    "    'San Francisco': [37.7749,-122.4194],\n",
    "    'Washington D.C': [38.8951,-77.0364]\n",
    "}\n",
    "\n",
    "# Collect data\n",
    "hourly_data_list = []\n",
    "daily_data_list = []\n",
    "\n",
    "for location, coords in location_map.items():\n",
    "    params = {\n",
    "        \"latitude\": coords[0],\n",
    "        \"longitude\": coords[1],\n",
    "        \"hourly\": [\"temperature_2m\", \"rain\", \"snow_depth\", \"cloud_cover\"],\n",
    "        \"daily\": [\"temperature_2m_max\", \"temperature_2m_min\", \"uv_index_max\"],\n",
    "        \"timezone\": \"America/New_York\"\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]\n",
    "\n",
    "    # Hourly data\n",
    "    hourly = response.Hourly()\n",
    "    hourly_data = {\n",
    "        \"date\": pd.date_range(start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                              end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                              freq=pd.Timedelta(seconds=hourly.Interval()), inclusive=\"left\"),\n",
    "        \"temperature_2m\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "        \"rain\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "        \"snow_depth\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "        \"cloud_cover\": hourly.Variables(3).ValuesAsNumpy(),\n",
    "    }\n",
    "    hourly_df = pd.DataFrame(data=hourly_data)\n",
    "    hourly_df[\"location\"] = location\n",
    "    hourly_data_list.append(hourly_df)\n",
    "\n",
    "    # Daily data\n",
    "    daily = response.Daily()\n",
    "    daily_data = {\n",
    "        \"date\": pd.date_range(start=pd.to_datetime(daily.Time(), unit=\"s\", utc=True),\n",
    "                              end=pd.to_datetime(daily.TimeEnd(), unit=\"s\", utc=True),\n",
    "                              freq=pd.Timedelta(seconds=daily.Interval()), inclusive=\"left\"),\n",
    "        \"temperature_2m_max\": daily.Variables(0).ValuesAsNumpy(),\n",
    "        \"temperature_2m_min\": daily.Variables(1).ValuesAsNumpy(),\n",
    "        \"uv_index_max\": daily.Variables(2).ValuesAsNumpy(),\n",
    "    }\n",
    "    daily_df = pd.DataFrame(data=daily_data)\n",
    "    daily_df[\"location\"] = location\n",
    "    daily_data_list.append(daily_df)\n",
    "\n",
    "# Merge and format\n",
    "hourly_df = pd.concat(hourly_data_list, ignore_index=True)\n",
    "daily_df = pd.concat(daily_data_list, ignore_index=True)\n",
    "\n",
    "hourly_df[\"date_only\"] = hourly_df[\"date\"].dt.date\n",
    "hourly_df[\"time_only\"] = hourly_df[\"date\"].dt.time\n",
    "hourly_df[\"is_day\"] = 0\n",
    "hourly_df[\"time_only_dt\"] = pd.to_datetime(hourly_df[\"time_only\"], format=\"%H:%M:%S\")\n",
    "hourly_df.loc[(hourly_df[\"time_only_dt\"].dt.hour >= 0) & (hourly_df[\"time_only_dt\"].dt.hour < 12), \"is_day\"] = 1\n",
    "hourly_df = hourly_df.drop(columns=[\"time_only_dt\", \"date\"])\n",
    "\n",
    "daily_df[\"date_only\"] = daily_df[\"date\"].dt.date\n",
    "daily_df[\"time_only\"] = daily_df[\"date\"].dt.time\n",
    "daily_df = daily_df.drop(columns=[\"date\"])\n",
    "\n",
    "# Upload to S3\n",
    "csv_buffer_hourly = StringIO()\n",
    "csv_buffer_daily = StringIO()\n",
    "hourly_df.to_csv(csv_buffer_hourly, index=False)\n",
    "daily_df.to_csv(csv_buffer_daily, index=False)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"weather-data-bucket-test1\"\n",
    "folder = \"rawData/\"\n",
    "s3.put_object(Bucket=bucket, Key=f\"{folder}hourly_data.csv\", Body=csv_buffer_hourly.getvalue())\n",
    "s3.put_object(Bucket=bucket, Key=f\"{folder}daily_data.csv\", Body=csv_buffer_daily.getvalue())\n",
    "print(\"Uploaded both CSVs to S3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3e4a0",
   "metadata": {},
   "source": [
    "## 🛠️ Step 2: Convert CSVs to Parquet using PySpark\n",
    "This script reads the uploaded raw CSV files, transforms them into efficient Parquet format, and saves the results in `s3://weather-data-bucket-test1/processedData/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ec2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV_to_Parquet_Converter\").getOrCreate()\n",
    "\n",
    "S3_RAW_PATH = \"s3://weather-data-bucket-test1/rawData/\"\n",
    "S3_PROCESSED_PATH = \"s3://weather-data-bucket-test1/processedData/\"\n",
    "\n",
    "hourly_df = spark.read.option(\"header\", \"true\").csv(S3_RAW_PATH + \"hourly_data.csv\")\n",
    "daily_df = spark.read.option(\"header\", \"true\").csv(S3_RAW_PATH + \"daily_data.csv\")\n",
    "\n",
    "hourly_df.write.mode(\"overwrite\").parquet(S3_PROCESSED_PATH + \"hourly/\")\n",
    "daily_df.write.mode(\"overwrite\").parquet(S3_PROCESSED_PATH + \"daily/\")\n",
    "\n",
    "print(\"CSV to Parquet conversion completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de2b3d",
   "metadata": {},
   "source": [
    "## 🔄 Step 3: Crawler & Athena\n",
    "- An AWS Glue Crawler scans the `processedData/` Parquet files and maps schema into a Glue Data Catalog.\n",
    "- Amazon Athena then queries the tables `plshourly` and `plsdaily` from the catalog.\n",
    "- One important thing to ensure in this step is that the workgroup needs a valid query result S3 location configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f802a22f",
   "metadata": {},
   "source": [
    "## 📊 Step 4: Power BI Integration\n",
    "- Now using the Athena ODBC driver in Power BI we can connect to Athena.\n",
    "- Load the `plsdaily` and `plshourly` tables.\n",
    "- Consume the data in the form of dashboards with drill-downs, trend charts, and daily/hourly comparisons on Power BI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
